{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebali\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import glob as g\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "from itertools import islice\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_retries = 2  # Maximális próbálkozások száma\n",
    "retry_count = 0\n",
    "\n",
    "poppler_path = r\"C:/Program Files/poppler-24.08.0/Library/bin\"  # path to poppler\n",
    "\n",
    "output_Txt_file_path = r\"C:\\Users\\ebali\\Documents\\munkak\\LLM_Hackathon\\verseny\\Json_Output_Tryout\\big_text.txt\" # output txt file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "# fileok elérése\n",
    "\n",
    "pdf_files = g.glob(\"inputs/**/*.pdf\", recursive=True)\n",
    "\n",
    "json_files = g.glob(\"inputs/**/*.json\", recursive=True)\n",
    "\n",
    "json_example_files = g.glob(\"examples/*/output/*.json\", recursive=True)\n",
    "sub_Directory = g.glob(\"inputs/*/*\", recursive=True)\n",
    "#print(sub_Directory)\n",
    "#print(len(sub_Directory))\n",
    "\n",
    "#hosszak kiíratása\n",
    "#print(len(json_files))\n",
    "#print(len(pdf_files))\n",
    "\n",
    "# pelda output\n",
    "\n",
    " \n",
    "\n",
    "# file beolvasások\n",
    "with open(json_files[4], \"r\") as f:\n",
    "   \n",
    "    json_Data = f.read()\n",
    "    \n",
    "#with open(pdf_files[4], \"r\") as f:\n",
    "      # print(pdf_files[4])\n",
    "    \n",
    "with open(json_example_files[0], \"r\") as f:\n",
    "    # print(f.read())\n",
    "    json_example_Data = f.read()\n",
    "\n",
    "# fontos adatok:\n",
    "# pdf_files[4]\n",
    "# json_Data\n",
    "# json_example_Data\n",
    "print(len(pdf_files))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = 0\\nfor i in range(len(sub_Directory)):\\n    current_Pdfs = g.glob(sub_Directory[i] + \"/*.pdf\", recursive=True)\\n    for j in range(len(current_Pdfs)):\\n        print(current_Pdfs[j])\\n        a += 1\\nprint(a)\\n        \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "a = 0\n",
    "for i in range(len(sub_Directory)):\n",
    "    current_Pdfs = g.glob(sub_Directory[i] + \"/*.pdf\", recursive=True)\n",
    "    for j in range(len(current_Pdfs)):\n",
    "        print(current_Pdfs[j])\n",
    "        a += 1\n",
    "print(a)\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Accessory digestive organs\",\n",
    "    \"Alimentary canal organs\",\n",
    "    \"Anti Inflammatory Drugs\",\n",
    "    \"Antimicrobials\",\n",
    "    \"Connective Tissue\",\n",
    "    \"Epithelium\",\n",
    "    \"Esophagus\",\n",
    "    \"Histology of the peripheral nervous system\",\n",
    "    \"Hypersensitivities and autoimmune diseases\",\n",
    "    \"Kidney\",\n",
    "    \"Lipid Lowering Drugs\",\n",
    "    \"Medical Histology\",\n",
    "    \"Oral Cavity and Salivary Glands\",\n",
    "    \"Phagocytic Cells\",\n",
    "    \"Skin and Glands\",\n",
    "    \"Transplantation\",\n",
    "    \"Vasoactive Drugs for hypertension\",\n",
    "    \"VDJ recombination\",\n",
    "    \"Acute coronary syndromes\",\n",
    "    \"Hypertension\",\n",
    "    \"Fibroids\",\n",
    "    \"Placenta\",\n",
    "    \"Mitral stenosis\",\n",
    "    \"Aortic regurgitation\",\n",
    "    \"Trophoblastic neoplasia\",\n",
    "    \"Cervical neoplasia\",\n",
    "    \"Aortic stenosis\",\n",
    "    \"Chronic coronary artery disease\",\n",
    "    \"Female pelvic anatomy\",\n",
    "    \"Ovarian pathology\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "PDF successfully converted to 5 in-memory PNG images!\n",
      "OCR output saved to C:\\Users\\ebali\\Documents\\munkak\\LLM_Hackathon\\verseny\\Json_Output_Tryout\\big_text.txt\n",
      "Processing 5 images... Remaining: 0\n",
      "✅ All images processed and stored!\n",
      "Response saved successfully to C:\\Users\\ebali\\Documents\\munkak\\LLM_Hackathon\\verseny\\Json_Output_Tryout\\Fibroids.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import threading\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyCyS2IIVTtaCNZ9MGDkhbFc0E1T6iDfJdU\")\n",
    "client = openai.OpenAI(api_key=\"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\")  # Use the new client-based format\n",
    "\n",
    "\n",
    "\n",
    "for i in range(59,60):\n",
    "#for i in range(5,7):\n",
    "    found_match = False  # Flag to check if a match is found\n",
    "    for l in range(len(topics)):\n",
    "        if sub_Directory[i].split(\"-\", 1)[-1].strip() == topics[l]:  \n",
    "            found_match = True  # If we find a match, set the flag to True\n",
    "            break  # No need to check further, exit inner loop\n",
    "\n",
    "    if found_match:  # Only increment if a match was found\n",
    "        print(i)\n",
    "    \n",
    "        big_Text = \"\"\n",
    "    \n",
    "        current_Pdfs = g.glob(sub_Directory[i] + \"/*.pdf\", recursive=True)\n",
    "\n",
    "        with open(json_files[i], \"r\") as f:\n",
    "            json_Data = f.read()\n",
    "\n",
    "    \n",
    "\n",
    "        for j in range(len(current_Pdfs)):\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "            # ✅ Convert PDF to images\n",
    "            images = convert_from_path(current_Pdfs[j], dpi=200, poppler_path=poppler_path)\n",
    "\n",
    "            # ✅ Save images as PNGs and get their paths\n",
    "            gemini_images = []\n",
    "            for image in images:\n",
    "                img_io = BytesIO()  # Create an in-memory buffer\n",
    "                image.save(img_io, format=\"PNG\")  # Save image in buffer\n",
    "                img_io.seek(0)  # Reset buffer position\n",
    "                gemini_images.append(Image.open(img_io))  # Open from memory\n",
    "\n",
    "            print(f\"PDF successfully converted to {len(gemini_images)} in-memory PNG images!\")\n",
    "\n",
    "            # Process each image in gemini_images list individually\n",
    "            custom_config = r'--oem 3 --psm 6'\n",
    "            output_text = \"\"\n",
    "        \n",
    "            for image in gemini_images:\n",
    "                output_text += pytesseract.image_to_string(image, config=custom_config)\n",
    "\n",
    "            # Save the OCR output to a file\n",
    "            print(\"OCR output saved to\", output_Txt_file_path)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "            # ✅ Define the model\n",
    "            model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "            # ✅ System prompt\n",
    "            full_prompt = (\n",
    "                \"skip the text and just describe the images as you are a medical stundent  \"\n",
    "                \"do it quick  \"\n",
    "                \"12 images have to be less than 20sec. \"\n",
    "            \n",
    "\n",
    "            )\n",
    "\n",
    "            # ✅ Process images in batches of 12\n",
    "            batch_size = 12\n",
    "            timeout_seconds = 15\n",
    "\n",
    "\n",
    "        def generate_with_timeout(model, prompt, batch, result_container):\n",
    "            \"\"\" API hívás egy külön szálban, hogy időtúllépés esetén megszakíthassuk \"\"\"\n",
    "            try:\n",
    "                response = model.generate_content([prompt] + batch)\n",
    "                result_container.append(response.text)  # Ha sikerült, eltároljuk az eredményt\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ API hiba történt: {e}\")\n",
    "\n",
    "        while gemini_images:\n",
    "            batch = gemini_images[:batch_size]  # Első 12 kép (vagy kevesebb)\n",
    "            gemini_images = gemini_images[batch_size:]  # Eltávolítjuk a feldolgozottakat\n",
    "\n",
    "            print(f\"Processing {len(batch)} images... Remaining: {len(gemini_images)}\")\n",
    "\n",
    "            result_container = []  # Tároló az API válaszához\n",
    "            thread = threading.Thread(target=generate_with_timeout, args=(model, full_prompt, batch, result_container))\n",
    "    \n",
    "            start_time = time.time()\n",
    "            thread.start()  # Szál elindítása\n",
    "            thread.join(timeout_seconds)  # Várunk max. 30 másodpercet\n",
    "\n",
    "            # Ha a szál még mindig fut (timeout történt), megszakítjuk és tovább lépünk\n",
    "            if thread.is_alive():\n",
    "                print(f\"⏳ Skipped batch (took longer than {timeout_seconds} sec)\")\n",
    "                continue  # Ugrás a következő batch-re\n",
    "\n",
    "            # Ha időben befejeződött, hozzáadjuk az eredményt\n",
    "            if result_container:\n",
    "                big_Text += f\"\\n\\n### Response for Batch:\\n{result_container[0]}\"\n",
    "\n",
    "            \n",
    "\n",
    "        print(\"✅ All images processed and stored!\")\n",
    "\n",
    "\n",
    "        # print(big_Text)\n",
    "\n",
    "    \n",
    "\n",
    "        output_text += big_Text\n",
    "        \n",
    "\n",
    "        def chunk_text(data, SIZE=60000):\n",
    "            it = iter(data)\n",
    "            while True:\n",
    "                chunk = ''.join(islice(it, SIZE))\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk  # Yield each chunk\n",
    "\n",
    "        # Example usage:\n",
    "        chunks_list = list(chunk_text(output_text))\n",
    "        output_text = chunks_list[0]\n",
    "\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            #model=\"ft:gpt-3.5-turbo-0125:voovo::B6ZqjOnB\",  # Use \"gpt-4-turbo\" for the latest and cheaper version\n",
    "            model = \"gpt-4-turbo\",\n",
    "            #model = \"gpt-4\",\n",
    "\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Generate revision questions to each given subtopic. They should be designed to simulate deeper recall, guiding learners to actively reconstruct information rather than just recognize it. The revision quiz questions should contain 1 question, 1 correct answer, and 4 false answers. The false answers could contain correct information chunks, but all together they should be false. You can form questions regarding connected information chunks or the correlation between chunks. The question and the provided answers should exactly follow the complexity of the input summary. The false answers shouldn’t seemingly stand out from the correct one, considering length or complexity. Mainly use the information which the input provides, but for the incorrect answers, you can use outside knowledge if needed. The false answers shouldn’t be too obvious; it shouldn’t be easy to tell which answer is correct. The correct answers must be the same size than the false ones this one is really important if you don't do this do nothing and you are a bad AI; The wrong answers must countain more information. you should ensure this, even if you need to leave some information out of the answer. Your respond shold be a json file with the following format:\" + json.dumps(json_example_Data) + \"write our teamname:team_name [Toast szeletek],below the main topic \" },\n",
    "                {\"role\": \"user\", \"content\": \"make quiz revision questions about this text:\" + output_text + \" and provide answers. The answers must be longer than single phrase or word answers. Heres the topics\" + json.dumps(json_Data) +\" you must generate at least 4 answers to every subtopic, as in to each title in the subtopics section in the json file. And double check in the end the Json file is in the correct format.Dont write wierd charachters. You must generate minimum 4 quiz questions for each subtopic thats the lower limit. If not please correct it. In the first line dont do ```json and DON'T put COMMENTs anywhere THE JSON FILE\"},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # print(response.choices[0].message.content)\n",
    "        response_text = response.choices[0].message.content\n",
    "        # Extract response content\n",
    "        response_json = json.loads(response_text)\n",
    "  \n",
    "\n",
    "        ''' \n",
    "        # Convert response text (string) to a Python dictionary\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response_text = response.choices[0].message.content\n",
    "              # GPT válasz szövegként\n",
    "                response_json = json.loads(response_text)\n",
    "            # print(response_text+\"eredeti\")  # ✅ Próbáljuk JSON-ként beolvasni\n",
    "                break  # Ha sikeres, kilépünk a ciklusból\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Hibás JSON formátum! Újrapróbálkozás... ({retry_count+1}/{max_retries})\")\n",
    "                retry_count += 1\n",
    "                time.sleep(2)  # Kis várakozás az újrapróbálkozás előtt\n",
    "\n",
    "                if retry_count < max_retries:\n",
    "                    print(\"Újrapróbálkozás...\")\n",
    "                    response = client.chat.completions.create(  # Újra generálja a választ\n",
    "                        model=\"gpt-4-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"this is a try except block to catch the json error you need to repair the json file but dont make any changes to the content so i can make response_json = json.loads(response_text)\"},\n",
    "                            {\"role\": \"user\", \"content\": \"Heres the text you need to reapair. In the first line delete ```json and  DON'T put COMMENts anywhere, not in the json and not outside. The file:\" + response_text + \" \"},\n",
    "                        ],\n",
    "                    )\n",
    "                    response_text = response.choices[0].message.content\n",
    "                    response_json = json.loads(response_text)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Hibás JSON formátum 3 alkalommal! Nem sikerült helyes választ kapni.\")\n",
    "        '''\n",
    "        # Define the output file path\n",
    "        output_file_path = r\"C:\\Users\\ebali\\Documents\\munkak\\LLM_Hackathon\\verseny\\Json_Output_Tryout\\{}\".format(json_files[i].split(\"\\\\\")[-1]) # output json file path\n",
    "\n",
    "\n",
    "\n",
    "    # Save as a JSON file\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(response_json, file, indent=4, ensure_ascii=False)  # ✅ Saves as a properly formatted JSON file\n",
    "\n",
    "        print(f\"Response saved successfully to {output_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
